{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-06T18:27:16.075937Z",
     "start_time": "2025-01-06T18:27:13.089963Z"
    }
   },
   "source": [
    "import tensorflow as tf \n",
    "print(tf.__version__)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T18:27:16.214039Z",
     "start_time": "2025-01-06T18:27:16.203969Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "397510dfd5aacd0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Preprocessing of data \n",
    "\n",
    "In this part we will get the necessary data for training/test/validation parts and check if the data has : \n",
    "- zeros (lack of value) \n",
    "- shape of data "
   ],
   "id": "4be6799f2f6a978d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T18:28:38.364309Z",
     "start_time": "2025-01-06T18:28:38.267093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "train=pd.read_table('../data/train.txt', delimiter =';', header=None, )\n",
    "val=pd.read_table('../data/val.txt', delimiter =';', header=None, )\n",
    "test=pd.read_table('../data/test.txt', delimiter =';', header=None, )\n",
    "\n",
    "data = pd.concat([train ,  val , test])\n",
    "data.columns = [\"text\", \"label\"]\n",
    "print(data)\n",
    "\n"
   ],
   "id": "568d3e13568528ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text    label\n",
      "0                               i didnt feel humiliated  sadness\n",
      "1     i can go from feeling so hopeless to so damned...  sadness\n",
      "2      im grabbing a minute to post i feel greedy wrong    anger\n",
      "3     i am ever feeling nostalgic about the fireplac...     love\n",
      "4                                  i am feeling grouchy    anger\n",
      "...                                                 ...      ...\n",
      "1995  i just keep feeling like someone is being unki...    anger\n",
      "1996  im feeling a little cranky negative after this...    anger\n",
      "1997  i feel that i am useful to my people and that ...      joy\n",
      "1998  im feeling more comfortable with derby i feel ...      joy\n",
      "1999  i feel all weird when i have to meet w people ...     fear\n",
      "\n",
      "[20000 rows x 2 columns]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T18:28:50.413580Z",
     "start_time": "2025-01-06T18:28:50.390182Z"
    }
   },
   "cell_type": "code",
   "source": "data.shape",
   "id": "f2cfe77e67046e87",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T18:28:52.079361Z",
     "start_time": "2025-01-06T18:28:52.051265Z"
    }
   },
   "cell_type": "code",
   "source": "data.isna().any(axis=1).sum()",
   "id": "f02146651a2b6982",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Stemming of the text\n",
    "\n",
    "Here we need to preprocess each line of text using steeming "
   ],
   "id": "9df406167abf758d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T18:28:55.342160Z",
     "start_time": "2025-01-06T18:28:53.926861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re \n",
    "\n",
    "#text preprocessing\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def preprocess(line):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', line) #leave only characters from a to z\n",
    "    review = review.lower() #lower the text\n",
    "    review = review.split() #turn string into list of words\n",
    "    #apply Stemming \n",
    "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')] #delete stop words like I, and ,OR   review = ' '.join(review)\n",
    "    #trun list into sentences\n",
    "    return \" \".join(review)"
   ],
   "id": "1420cb3c01a8cd87",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We apply the stemming process over all the data ",
   "id": "ed723bc739e128e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T18:30:29.251508Z",
     "start_time": "2025-01-06T18:28:58.345949Z"
    }
   },
   "cell_type": "code",
   "source": "data['text']=data['text'].apply(lambda x: preprocess(x))",
   "id": "17c5c7c45addfdc3",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we will transform categorical labels into numerical labels.",
   "id": "d659f0790012b79d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T18:30:29.515734Z",
     "start_time": "2025-01-06T18:30:29.491025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn import preprocessing  # Importing the preprocessing module from scikit-learn to use preprocessing tools\n",
    "\n",
    "# Creating a LabelEncoder object to convert categorical labels into numerical labels\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "# Using the encoder to transform the data in the 'label' column into numerical values\n",
    "# 'fit_transform' learns the unique categories and converts them into integers\n",
    "data['N_label'] = label_encoder.fit_transform(data['label'])\n",
    "\n",
    "# A new column 'N_label' is added to the DataFrame 'data', containing the encoded labels as numbers\n"
   ],
   "id": "549c906092e78730",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T18:30:29.595248Z",
     "start_time": "2025-01-06T18:30:29.555402Z"
    }
   },
   "cell_type": "code",
   "source": "data",
   "id": "b904134112d913ba",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                   text    label  N_label\n",
       "0                                     didnt feel humili  sadness        4\n",
       "1     go feel hopeless damn hope around someon care ...  sadness        4\n",
       "2                  im grab minut post feel greedi wrong    anger        0\n",
       "3        ever feel nostalg fireplac know still properti     love        3\n",
       "4                                          feel grouchi    anger        0\n",
       "...                                                 ...      ...      ...\n",
       "1995  keep feel like someon unkind wrong think get b...    anger        0\n",
       "1996            im feel littl cranki neg doctor appoint    anger        0\n",
       "1997              feel use peopl give great feel achiev      joy        2\n",
       "1998  im feel comfort derbi feel though start step s...      joy        2\n",
       "1999  feel weird meet w peopl text like dont talk fa...     fear        1\n",
       "\n",
       "[20000 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>N_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>didnt feel humili</td>\n",
       "      <td>sadness</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>go feel hopeless damn hope around someon care ...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grab minut post feel greedi wrong</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ever feel nostalg fireplac know still properti</td>\n",
       "      <td>love</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>feel grouchi</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>keep feel like someon unkind wrong think get b...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>im feel littl cranki neg doctor appoint</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>feel use peopl give great feel achiev</td>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>im feel comfort derbi feel though start step s...</td>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>feel weird meet w peopl text like dont talk fa...</td>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T18:30:32.359537Z",
     "start_time": "2025-01-06T18:30:29.693110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Importing CountVectorizer from scikit-learn to create a Bag of Words (BoW) model\n",
    "# BoW converts textual data into numerical data by counting word occurrences\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initializing CountVectorizer with specific parameters:\n",
    "# - max_features=5000: Limit the vocabulary to the 5000 most frequent words or n-grams\n",
    "# - ngram_range=(1,3): Extract unigrams (single words), bigrams (two consecutive words), and trigrams (three consecutive words)\n",
    "cv = CountVectorizer(max_features=5000, ngram_range=(1, 3))\n",
    "\n",
    "# Example: For the text \"the course was long\", the resulting n-grams will include:\n",
    "# ['the', 'the course', 'the course was', 'course', 'course was', 'course was long', 'was', 'was long', 'long']\n",
    "\n",
    "# Transforming the 'text' column in the dataset into a numerical matrix:\n",
    "# - fit_transform learns the vocabulary from the text and transforms each document into a numerical vector\n",
    "# - toarray converts the sparse matrix into a dense NumPy array\n",
    "data_cv = cv.fit_transform(data['text']).toarray()\n",
    "\n",
    "# The resulting 'data_cv' is a matrix where:\n",
    "# - Each row corresponds to a document in the dataset\n",
    "# - Each column corresponds to a word or n-gram in the vocabulary\n",
    "# - The values represent the count of occurrences of the word or n-gram in the document\n"
   ],
   "id": "df0b55fa200108c",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We split data into two sets : training and test",
   "id": "635b90727233c1fb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T18:30:35.160085Z",
     "start_time": "2025-01-06T18:30:32.439969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#X_train, X_test, y_train, y_test=data_cv,test_cv,train['N_label'],test['N_label']\n",
    "X_train, X_test, y_train, y_test =train_test_split(data_cv, data['N_label'], test_size=0.25, random_state=42)"
   ],
   "id": "dbf7cc488c33bac4",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Naive model\n",
    "We will in this model doesn't try to search for the best hyperparameters"
   ],
   "id": "6f40e790cc9589c9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T18:31:21.295207Z",
     "start_time": "2025-01-06T18:30:35.355888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (Dense)\n",
    "\n",
    "# load the dataset\n",
    "# split into input (X) and output (y) variables\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "# compile the keras model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=10)\n",
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(X_train, y_train)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ],
   "id": "3e3884a456d6a296",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1500/1500 [==============================] - 6s 3ms/step - loss: 0.9289 - accuracy: 0.6728\n",
      "Epoch 2/10\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.2957 - accuracy: 0.9044\n",
      "Epoch 3/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.1565 - accuracy: 0.9482\n",
      "Epoch 4/10\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.0957 - accuracy: 0.9702\n",
      "Epoch 5/10\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0680 - accuracy: 0.9787\n",
      "Epoch 6/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.0475 - accuracy: 0.9861\n",
      "Epoch 7/10\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0337 - accuracy: 0.9903\n",
      "Epoch 8/10\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0275 - accuracy: 0.9919\n",
      "Epoch 9/10\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.0224 - accuracy: 0.9931\n",
      "Epoch 10/10\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0187 - accuracy: 0.9951\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0134 - accuracy: 0.9959\n",
      "Accuracy: 99.59\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T18:31:21.894824Z",
     "start_time": "2025-01-06T18:31:21.337596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "_, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ],
   "id": "82583835a36ed82",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 3ms/step - loss: 0.9028 - accuracy: 0.8528\n",
      "Accuracy: 85.28\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T18:31:22.158250Z",
     "start_time": "2025-01-06T18:31:21.943806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "text='I feel Happy'\n",
    "text=preprocess(text)\n",
    "array = cv.transform([text]).toarray()\n",
    "pred = model.predict(array)\n",
    "a=np.argmax(pred, axis=1)\n",
    "label_encoder.inverse_transform(a)[0]"
   ],
   "id": "498221d7c90b1b46",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 131ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'joy'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We save the naive model",
   "id": "e911fa8f125b564a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T18:27:18.080319500Z",
     "start_time": "2025-01-06T05:12:44.302453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# create the repertory if needed\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "\n",
    "# save the model in the models repertory\n",
    "tf.keras.models.save_model(model, 'models/naive_model.h5')\n"
   ],
   "id": "8017ebe02035b027",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T18:27:18.080319500Z",
     "start_time": "2025-01-06T06:42:32.354397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "pickle.dump(label_encoder, open('../utils/encoder.pkl', 'wb'))\n",
    "pickle.dump(cv, open('../utils/CountVectorizer.pkl', 'wb'))"
   ],
   "id": "33b7319551bed961",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Study on the number of word to obtain a good guess",
   "id": "223e613b6d7b834f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T18:37:38.994438Z",
     "start_time": "2025-01-06T18:33:37.380323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fonction pour obtenir la prédiction du modèle\n",
    "def model_predict(text):\n",
    "    text = preprocess(text)  # Prétraitement\n",
    "    array = cv.transform([text]).toarray()  # Transformation en vecteur\n",
    "    pred = model.predict(array,verbose=0)  # Prédiction\n",
    "    a = np.argmax(pred, axis=1)  # Obtenir l'index de la classe prédite\n",
    "    return label_encoder.inverse_transform(a)[0]  # Décoder la classe prédite\n",
    "\n",
    "# Fonction pour tester à partir de combien de mots le modèle devine correctement\n",
    "def test_prediction_threshold(dataframe):\n",
    "    results = []\n",
    "    for _, row in dataframe.iterrows():\n",
    "        text = row['text']\n",
    "        true_label = row['label']\n",
    "        words = text.split()\n",
    "        correct_at = None  # Enregistre le seuil où la prédiction est correcte\n",
    "\n",
    "        for i in range(1, len(words) + 1):\n",
    "            subset = \" \".join(words[:i])  # Texte partiel avec i mots\n",
    "            prediction = model_predict(subset)\n",
    "            if prediction == true_label:\n",
    "                correct_at = i\n",
    "                break\n",
    "\n",
    "        results.append({\n",
    "            \"text\": text,\n",
    "            \"label\": true_label,\n",
    "            \"pred_correct_at_words\": correct_at\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Appliquer la fonction au DataFrame\n",
    "#results_df = test_prediction_threshold(data)\n",
    "# Appliquer la fonction uniquement à la première ligne du DataFrame\n",
    "rows = data.iloc[:1000]  # Extraire la première ligne\n",
    "\n",
    "# Passer la première ligne à la fonction\n",
    "result_rows = test_prediction_threshold(rows)\n",
    "\n",
    "# Afficher le résultat\n",
    "print(result_rows)\n",
    "\n",
    "\n",
    "# Afficher les résultats\n",
    "#print(results_df)"
   ],
   "id": "3810744a9023c124",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text    label  \\\n",
      "0                                    didnt feel humili  sadness   \n",
      "1    go feel hopeless damn hope around someon care ...  sadness   \n",
      "2                 im grab minut post feel greedi wrong    anger   \n",
      "3       ever feel nostalg fireplac know still properti     love   \n",
      "4                                         feel grouchi    anger   \n",
      "..                                                 ...      ...   \n",
      "995                         depress actual feel inspir      joy   \n",
      "996  feel like enough peopl age actual think pretti...  sadness   \n",
      "997           get home laze around pajama feel grouchi    anger   \n",
      "998                       feel pretti homesick weekend  sadness   \n",
      "999  start feel realli optimist driven paper coz go...      joy   \n",
      "\n",
      "     pred_correct_at_words  \n",
      "0                      3.0  \n",
      "1                      3.0  \n",
      "2                      1.0  \n",
      "3                      3.0  \n",
      "4                      2.0  \n",
      "..                     ...  \n",
      "995                    4.0  \n",
      "996                    9.0  \n",
      "997                    2.0  \n",
      "998                    3.0  \n",
      "999                    1.0  \n",
      "\n",
      "[1000 rows x 3 columns]\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T19:33:45.060905Z",
     "start_time": "2025-01-06T19:33:44.297840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sauvegarder les résultats en CSV\n",
    "result_rows.to_csv('../data/result_rows.csv', index=False)\n",
    "\n",
    "print(\"results have been saved in 'result_rows.csv'.\")"
   ],
   "id": "bbd5e969d6b9db38",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results have been saved in 'result_rows.csv'.\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## K cross validation model\n",
   "id": "d9e06597bbe161f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T18:27:18.083671100Z",
     "start_time": "2025-01-06T04:53:24.702381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Fonction pour créer un modèle Keras\n",
    "def create_model(optimizer='adam', neurons=12):\n",
    "    model_local = Sequential()\n",
    "    model_local.add(Dense(neurons, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "    model_local.add(Dense(8, activation='relu'))\n",
    "    model_local.add(Dense(6, activation='softmax'))\n",
    "    model_local.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model_local\n",
    "\n",
    "# Créer une classe personnalisée avec les mixins de scikit-learn\n",
    "class CustomKerasClassifier(KerasClassifier, BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"A KerasClassifier that explicitly inherits from BaseEstimator and ClassifierMixin.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Créer une instance de la classe personnalisée\n",
    "model = CustomKerasClassifier(model=create_model, epochs=10, batch_size=10, verbose=0)\n",
    "\n",
    "# Définir la grille d'hyperparamètres à rechercher\n",
    "param_grid = {\n",
    "    'model__optimizer': ['adam', 'sgd'],  # Optimizers à tester\n",
    "    'model__neurons': [12, 14, 16],        # Nombre de neurones dans la première couche\n",
    "    'batch_size': [10, 20],               # Tailles de batch à tester\n",
    "    'epochs': [10, 20],                   # Nombres d'époques\n",
    "}\n",
    "\n",
    "# Définir le nombre de K pour la cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Recherche des meilleurs hyperparamètres avec GridSearchCV\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=kfold, verbose=2)\n",
    "\n",
    "# Entraîner le modèle avec la recherche de grille\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Afficher les meilleurs hyperparamètres trouvés\n",
    "print(\"Best Hyperparameters:\", grid_result.best_params_)\n",
    "\n",
    "# Évaluer le modèle avec les meilleurs hyperparamètres\n",
    "best_model = grid_result.best_estimator_\n",
    "accuracy = best_model.score(X_test, y_test)\n",
    "print(f\"Test accuracy: {accuracy*100:.2f}%\")\n"
   ],
   "id": "dbe95795d1709a93",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Best Hyperparameters: {'model__neurons': 12, 'model__optimizer': 'sgd'}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CustomKerasClassifier' object has no attribute 'evaluate'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[42], line 46\u001B[0m\n\u001B[0;32m     44\u001B[0m \u001B[38;5;66;03m# Évaluer le modèle avec les meilleurs hyperparamètres\u001B[39;00m\n\u001B[0;32m     45\u001B[0m best_model \u001B[38;5;241m=\u001B[39m grid_result\u001B[38;5;241m.\u001B[39mbest_estimator_\n\u001B[1;32m---> 46\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m \u001B[43mbest_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m(X_test, y_test)\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTest accuracy: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00maccuracy\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m100\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'CustomKerasClassifier' object has no attribute 'evaluate'"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T18:27:18.083671100Z",
     "start_time": "2025-01-06T05:00:16.568946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "accuracy = best_model.score(X_test, y_test)\n",
    "print(f\"Test accuracy: {accuracy*100:.2f}%\")"
   ],
   "id": "60b95ff0f01894f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 86.44%\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T18:27:18.084690700Z",
     "start_time": "2025-01-06T04:52:26.064572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "text='I am sad'\n",
    "text=preprocess(text)\n",
    "array = cv.transform([text]).toarray()\n",
    "pred = best_model.predict(array)\n",
    "label_encoder.inverse_transform(a)[0]"
   ],
   "id": "d721fa1242af3821",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'joy'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
